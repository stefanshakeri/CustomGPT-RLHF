# CustomGPT-RLHF
RLHF for a CustomGPT from OpenAI's CustomGPTs. 

## Purpose
Since Custom GPTs cannot be fine-tuned or called through an API, this RLHF system can't fine-tune the model. The 'Reinforcement Learning' part of RLHF would have to be done through manual changes to the Custom GPT prompt. However, this system is useful in evaluating if the Custom GPT is heading in the right direction, and can then be augmented manually. 

## Methods

### Vectorization
The directory ```vector``` contains the vectorization method of RLHF, where two LLM responses are evaluated in their similarity against an expert response through cosine similarity of their vectors. 

### Agentic Judge
The directory ```agent``` contains the agentic method of RLHF, where two LLM responses are evaulated in their similarity agains an expert response through a separate LLM agent acting as a judge. 

## Usage

### Input Dataset
Create a CSV file, contained in the ```data``` folder, with four features: 
1. ```question```: the question the LLM is being asked
2. ```llm_response_1```: the first response generated by the LLM
3. ```llm_response_2```: the second response generated by the LLM
4. ```expert_response```: the response supplied by some expert opinion

### Environment Variables
Create a ```.env``` file with the following variables:
```
OPENAI_API_KEY='[your OpenAI API key]'
INPUT_FILE='[the name of your input CSV file, including the ".csv" portion]'
CHROMA_PATH='[the path name of where you want to store your local Chroma database]'
OUTPUT_FILE='[the name of your output CSV file, including the ".csv" portion]'
GPT_MODEL='[OpenAI model being used]'
```