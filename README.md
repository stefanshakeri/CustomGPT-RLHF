# CustomGPT-RLHF
RLHF for a CustomGPT from OpenAI's CustomGPTs. 

## Purpose
Since Custom GPTs cannot be fine-tuned or called through an API, this RLHF system can't fine-tune the model. The 'Reinforcement Learning' part of RLHF would have to be done through manual changes to the Custom GPT prompt. However, this system is useful in evaluating if the Custom GPT is heading in the right direction, and can then be augmented manually. 

## Functionality

### Input Dataset
The CSV dataset used, contained in the ```data``` folder, contains four features: 
1. ```question```: the question the LLM is being asked
2. ```llm_response_1```: the first response generated by the LLM
3. ```llm_response_2```: the second response generated by the LLM
4. ```expert_response```: the response supplied by some expert opinion

### Expert Response Vectorization
The expert responses will be static throughout testing, so they will be vectorized and uploaded to a Chroma vector database separately. Once they are vectorized, they can be compared through cosine similarity to the LLM responses to evalute their effectiveness. 

### LLM Response Vectorization
The two LLM responses will be vectored so that they can be compared to the expert responses through cosine similarity to evaluate the similarity of the responses. 

### Responses Comparison
The LLM responses and expert responses will be compared through cosine similarity to evaluate the strength of the LLM responses to determine which one was superior. A score will then be applied to each LLM response and outputted in a CSV. 